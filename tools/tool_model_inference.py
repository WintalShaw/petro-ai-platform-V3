# tools/tool_model_inference.py
import streamlit as st
import time
import random


def run(context):
    # è¿™é‡Œä¸éœ€è¦sleepï¼Œå› ä¸ºæˆ‘ä»¬åœ¨viewé‡Œæ¨¡æ‹Ÿè¿›åº¦æ¡
    return "æ¨ç†å¯åŠ¨"


def view(context):
    st.write("ğŸ§  æ­£åœ¨åŠ è½½ LSTM-Transformer æ··åˆæ¨¡å‹...")

    # æ¨¡æ‹Ÿä¸€ä¸ªæ¨ç†è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()

    # è¿›åº¦æ¡è·‘åŠ¨é€»è¾‘
    for i in range(101):
        if i % 10 == 0:  # åŠ å¿«ä¸€ç‚¹é€Ÿåº¦ï¼Œæ¯10%åœé¡¿ä¸€ä¸‹
            time.sleep(0.02)
            progress_bar.progress(i)
            # åŠ¨æ€æ˜¾ç¤ºæ¨ç†ç™¾åˆ†æ¯”
            status_text.text(f"Tensor Core æ¨ç†ä¸­... {i}%")

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šç”Ÿæˆéšæœºæ•°æ® ---
    # è€—æ—¶: 0.8 ~ 2.5 ç§’
    cost_time = random.uniform(0.8, 2.5)
    # æ˜¾å­˜: 3.5 ~ 6.2 GB
    vram_usage = random.uniform(3.5, 6.2)

    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€ (ä¿ç•™ä¸€ä½æˆ–ä¸¤ä½å°æ•°)
    status_text.text(f"âœ… æ¨¡å‹æ¨ç†å®Œæˆ | è€—æ—¶: {cost_time:.2f}s | æ˜¾å­˜å ç”¨: {vram_usage:.1f}GB")